# Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World

## 摘要
腿式机器人具有穿越各种复杂环境的物理能力，但设计足够强大的控制器以应对这种多样性一直是机器人学中的长期挑战。强化学习（RL）为自动化控制器设计过程提供了一种诱人的方法，当在合适的环境中训练时，RL可以生成非常强大的控制器。然而，预测机器人在部署过程中可能遇到的所有情况并在训练时将其列举出来是困难的。本文提出了一种实用的机器人强化学习系统，用于在现实世界中微调运动策略。我们展示了少量的现实世界训练可以显著提高部署期间的性能，这使得A1四足机器人能够在包括户外草坪和各种室内地形在内的多种环境中自动微调多种运动技能。

## I. 介绍
腿式机器人能够穿越各种环境和地形，从地下碎石到雪山。然而，要充分发挥这种能力，需要能够有效处理这些广泛环境的控制器。为每个机器人设计这样强大的控制器是一项劳动密集型的过程，通常需要人类专家的参与和对系统动态的精确建模。尽管强化学习算法已经被用于自动学习机器人在模拟和现实世界中的运动技能，但为了让这些方法在测试时处理机器人可能遇到的所有环境，它们必须在广泛的条件下进行训练。本文的目标是设计一个完整的系统，以在现实世界条件下微调机器人运动策略。

## II. 相关工作
传统的机器人运动控制器通常通过步态规划、轨迹优化和模型预测控制（MPC）相结合来构建。然而，这种方法要求对机器人动态的精确表征，并且通常需要为每个机器人和每种行为进行大量的手动设计。RL提供了一种有吸引力的方法，能够在模拟和现实世界中学习这些技能。然而，由于安全考虑和RL算法对数据的密集需求，RL运动控制器通常在模拟中进行训练，之后转移到现实世界。这种方法存在的主要挑战是如何在现实世界中进行有效的适应和微调。

## III. 在现实世界中微调运动
### a 概述
我们提出了一个用于微调运动策略的系统，它结合了稳定和高效的RL算法与多任务训练，从而使我们的机器人能够在最小的人类干预下快速学习。我们的方法包括在模拟中预训练技能，并在现实世界中通过RL算法继续训练以进行微调。

### b 动作模仿
我们通过模仿参考运动片段来训练不同的技能策略。这种方法允许我们通过交换参考运动来学习不同的技能。

### c 离线策略强化学习
我们采用了REDQ算法，这是一种对SAC算法的简单扩展，允许更高的梯度步骤与时间步长的比率，从而提高了样本效率。

## IV. 系统设计
### IV-A 状态和动作空间
状态包含机器人在每个时间步的根部姿态、关节角度和先前的动作。动作是对每个关节的PD位置目标，并以33Hz的频率应用。

### IV-B 奖励函数
我们采用了一个由多个组件组成的奖励函数，包括姿态奖励、速度奖励、末端效应器奖励、根部姿态奖励和根部速度奖励。该函数旨在通过鼓励机器人在每个时间步跟踪目标姿态来激励机器人执行所需的运动。

### IV-C 重置控制器
我们在模拟中通过生成多样的初始状态来训练重置策略。机器人需要从一个随机高度掉落并恢复到默认站立姿态。这个策略比手工设计的恢复方法更加通用和敏捷。

## V. 实验
### V-A 模拟实验
我们在模拟环境中评估了我们的方法，并与之前的方法进行了比较，特别是在适应多种测试环境方面。

### V-B 现实世界实验
我们的现实世界实验旨在评估自主在线微调如何提高机器人在各种现实环境中的技能表现。我们选择了四个现实世界场景，包括户外草坪、地毯房间、带裂缝的门垫和记忆泡沫。实验结果表明，通过在真实世界中微调，机器人在各个环境中的性能都有显著提升。

## VI. 结论
我们提出了一种系统，该系统使腿式机器人能够在现实世界中通过自主数据收集和高效的无模型RL进行运动策略微调。我们的系统能够自动恢复摔倒，利用车载传感器进行状态估计，并高效地微调各种运动技能。这种微调极大地提高了机器人的性能，即使在最初的策略不稳定时也能逐步优化表现。未来的工作将致力于开发终身学习系统，使机器人能够在遇到新环境时快速适应，并在熟悉的环境中逐步完善技能。


# 该篇文章的笔记

## 1. 该篇文章的研究目的
### 1.1 研究目的
本文旨在解决腿式机器人在各种复杂环境中进行自动化微调的问题。尽管强化学习（RL）已经被用来训练机器人在多种环境下的运动技能，但由于现实世界中环境的多样性，训练出的控制器往往难以泛化到所有的实际情况中。为此，本文提出了一种在现实世界中微调运动策略的RL系统，使得机器人能够在实际部署时继续学习和优化其运动技能，特别是在四足机器人上的应用。

## 2. 该篇文章的研究方法
### 2.1 预训练和现实世界微调
本文采用了结合模拟预训练和现实世界微调的方法。首先，使用模拟数据进行预训练，确保机器人在安全和高效的环境中学习初步的运动技能。接着，将这些策略应用于现实世界，并通过同样的RL算法在新环境中继续训练和微调，以适应现实世界中的复杂动态。

### 2.2 多任务学习和恢复控制器
为提高学习效率，本文使用了多任务框架，能够同时训练和微调多个技能（如前进和后退步态）。此外，设计了一个恢复控制器，使机器人在摔倒后能够自动恢复站立，避免了人为干预。

### 2.3 离线策略强化学习
本文采用了随机化集成双Q学习（REDQ）算法，这是一种高效的离线RL算法，可以在现实世界中以更高的样本效率进行训练。

## 3. 该篇文章的研究内容
### 3.1 状态和动作空间设计
状态包括根部姿态、关节角度和先前的动作，动作则是对每个关节的PD位置目标。为了确保动作的平滑性，使用低通滤波器处理PD目标。

### 3.2 奖励函数设计
奖励函数由多个部分组成，包括姿态奖励、速度奖励、末端效应器奖励、根部姿态奖励和根部速度奖励，以激励机器人在每个时间步跟踪目标姿态。

### 3.3 重置控制器设计
重置控制器通过生成多样的初始状态进行训练，使机器人在每个试验后能够自动恢复到默认站立姿态。这一控制器比手工设计的恢复方法更加通用和敏捷。

## 4. 该篇文章的最大创新点
### 4.1 创新点总结
本文最大的创新点在于提出了一种结合模拟预训练和现实世界微调的RL系统，用于在现实世界中持续改进腿式机器人的运动策略。通过设计自动恢复控制器和多任务学习框架，机器人能够在复杂多变的环境中自主学习和优化运动技能，从而显著提高了机器人在现实世界中的表现。

## 5. 该篇文章给我们的启发
### 5.1 启发总结
这项研究展示了如何通过结合模拟和现实世界训练，利用强化学习实现机器人运动技能的自主优化。该方法特别适用于需要在复杂多变环境中持续学习的应用场景，为未来的机器人研究提供了新的思路。本文的系统有望进一步发展为一种终身学习系统，使机器人能够在新环境中快速适应，并在熟悉的环境中逐步完善技能。
